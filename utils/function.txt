def _create_convnet_dataset(self):
        """
        Create a dataset that a convolutional network can learn on by corrupting randomly chosen messages
        and save it as pickle in utils/anomaly_detection_files/convnet_inputs.h5 
        Argument: data_original - object of a Data class, containing all 3 datasets (train, val, test) with:
          X, Xraw, message_bits, message_decoded, MMSI
        """
        file = h5py.File(name='data/Gdansk.h5', mode='r')
        data = Data(file=file)
        data.split(train_percentage=50, val_percentage=25) # split into train, val and test set
        file.close()
        # Compose a dataset from train and val sets, keep test untouched
        message_bits = []
        message_bits.append(data.message_bits_train)
        message_bits.append(data.message_bits_val)
        message_decoded = []
        message_decoded.append(data.message_decoded_train)
        message_decoded.append(data.message_decoded_val)
        Xraw = []
        Xraw.append(data.Xraw_train)
        Xraw.append(data.Xraw_val)
        MMSI = []
        MMSI.append(data.MMSI_train)
        MMSI.append(data.MMSI_val)
        field_bits = np.array([6, 8, 38, 42, 50, 60, 61, 89, 116, 128, 137, 143, 148])  # range of fields
        x = []
        x.append([])
        x.append([])
        y = []
        y.append([])
        y.append([])
        corruption = []
        corruption.append(Corruption(message_bits[0],1))
        corruption.append(Corruption(message_bits[1],1))
        clustering = Clustering() # First clustering
        idx = []
        idx.append(clustering.run_DBSCAN(X=data.normalize(Xraw[0])[0])[0])
        idx.append(clustering.run_DBSCAN(X=data.normalize(Xraw[1])[0])[0])
        for field in self.inside_fields:  # Corrupt the specified field
            for i in range(3000): # If i even add to train, if i odd to val
                stop = False
                while not stop:
                    message_idx = np.random.randint(message_bits[i%2].shape[0])
                    # If there is at least one message from the past
                    if len(np.where(np.array(MMSI[i%2]) == MMSI[i%2][message_idx])[0])>3:
                        # Choose a bit to corrupt (based on a range of the field)
                        bit = np.random.randint(field_bits[field-1], field_bits[field]-1)
                        # Corrupt that bit in a randomly chosen message
                        message_bits_corr, _ = corruption[i%2].corrupt_bits(
                            message_bits=message_bits[i%2],
                            bit_idx=bit,
                            message_idx=message_idx)
                        message_decoded_corr = copy.deepcopy(message_decoded[i%2])
                        X_corr = copy.deepcopy(Xraw[i%2])
                        X_0, _, message_decoded_0 = decode(message_bits_corr[message_idx,:])  # decode from binary             
                        message_decoded_corr[message_idx] = message_decoded_0
                        X_corr[message_idx] = X_0
                        # cluster again to find new cluster assignment
                        idx_corr, _ = clustering.run_DBSCAN(X=data.normalize(X_corr)[0])
                        # Check if the cluster is inside a proper cluster: if so, stop searching
                        stop = check_cluster_assignment(idx[i%2], idx_corr, message_idx)
                # Create a sample - take _sample_length consecutive examples
                sample = self.compute_inside_sample(
                    message_decoded=message_decoded_corr,
                    MMSI=MMSI[i%2],
                    message_idx=message_idx)
                x[i%2].append(sample)
                # Create a ground truth vectors y
                y[i%2].append(1)
                # Create a negative sample
                sample = self.compute_inside_sample(
                    message_decoded=copy.deepcopy(message_decoded[i%2]),
                    MMSI=MMSI[i%2],
                    message_idx=message_idx)
                x[i%2].append(sample)
                y[i%2].append(0)
                #sample_messages_idx = messages_idx[range(start_idx, stop_idx)]
                #sample_message_idx = np.where(sample_messages_idx==message_idx)[0]
                #vec_field = np.zeros((len(self.inside_fields)))
                #vec_field[self.inside_fields.index(field)]=1
                #y_field[i%2].append(vec_field)
        # Save file with the inputs for the classifier
        x_train, y_train = shuffle(x[0], y[0])
        x_val, y_val = shuffle(x[1], y[1])
        variables = [x_train, y_train, x_val, y_val]
        pickle.dump(variables, open('utils/anomaly_detection_files/convnet_inputs.h5', 'ab'))